services:
  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    environment:
      # Set your desired root user and password
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      # Optional: automatically create a default bucket on startup
      MINIO_DEFAULT_BUCKETS: my-first-bucket
    ports:
      # API port (e.g., for S3 clients)
      - "9000:9000"
      # Console UI port (e.g., for browser access)
      - "9090:9090"
    volumes:
      # Persist data to a local volume named 'minio_data'
      - minio_data:/data
    command: >
      server --console-address ":9090" /data
    networks:
      - spark-net

  iceberg:
    image: tabulario/iceberg-rest:latest
    ports:
      - "8181:8181"
    environment:
      #CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO

      MINIO_USER: minioadmin
      MINIO_PASSWORD: minioadmin
      MINIO_REGION: us-east-1

      CATALOG_S3_ACCESS__KEY__ID: minioadmin
      CATALOG_S3_SECRET__ACCESS__KEY: minioadmin
      CATALOG_S3_REGION: us-east-1

      AWS_REGION: us-east-1

      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      # === Catálogo JDBC (Postgres) ===
      CATALOG_CATALOG__IMPL: org.apache.iceberg.jdbc.JdbcCatalog
      CATALOG_URI: jdbc:postgresql://db:5432/iceberg
      CATALOG_JDBC_USER: iceberg
      CATALOG_JDBC_PASSWORD: iceberg
    networks:
      - spark-net
    depends_on:
      - db
      - minio

  trino:
    image: trinodb/trino:latest
    container_name: trino
    ports:
      - "8080:8080"
    depends_on:
      - iceberg
      - minio
    networks:
      - spark-net
    volumes:
      - ./trino/catalog:/etc/trino/catalog

  db:
    image: postgres:14
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: iceberg
      POSTGRES_USER: iceberg
      POSTGRES_PASSWORD: iceberg
    networks:
      - spark-net
    volumes:
      - pgdata:/var/lib/postgresql/data

  spark-master:
    image: apache/spark:3.5.0
    hostname: spark-master
    command: bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    ports:
      - "7077:7077"   # Porta para comunicação com workers
      - "8082:8080"   # UI do Spark Master
    environment:
      - SPARK_HOME=/opt/spark
    networks:
      - spark-net

  spark-worker:
    image: apache/spark:3.5.0
    # hostname: spark-worker-1
    command: bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    depends_on:
      - spark-master
    ports:
      - "8081:8081"   # UI do Spark Worker 1
    environment:
      - SPARK_HOME=/opt/spark
    networks:
      - spark-net

  #Parte 2 - Adicionando o Jupyter + Spark
  jupyter:
    image: jupyter/all-spark-notebook:latest
    container_name: jupyter
    hostname: jupyter
    ports:
      - "8888:8888"
    environment:
      - SPARK_HOME=/usr/local/spark  # Caminho correto
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./notebooks/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    depends_on:
      - spark-master
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

# Define the volume for persistent data
volumes:
  minio_data:
  pgdata:

