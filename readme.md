# ğŸš€ Data Lakehouse Pessoal â€” MinIO + Iceberg + Trino

Projeto de **Data Lakehouse pessoal** construÃ­do para ingestÃ£o, armazenamento, processamento e anÃ¡lise de dados financeiros, utilizando tecnologias modernas do ecossistema open source.

O ambiente integra armazenamento objeto, tabela analÃ­tica ACID, engine SQL distribuÃ­da e ferramentas de anÃ¡lise exploratÃ³ria.

---

## ğŸ§± Arquitetura

Stack utilizada:

* **Object Storage:** MinIO
* **Formato de Tabela Lakehouse:** Apache Iceberg (via REST Catalog)
* **Query Engine SQL:** Trino
* **BI / VisualizaÃ§Ã£o:** Metabase
* **Processamento DistribuÃ­do:** Apache Spark (PySpark)
* **Processamento DataFrame em memÃ³ria:** Polars
* **Ambiente de ExploraÃ§Ã£o:** Jupyter Notebook

---

## ğŸ—ï¸ VisÃ£o da Arquitetura

```text
                +----------------------+
                |      Metabase        |
                +----------+-----------+
                           |
                           v
                    +-------------+
                    |    Trino    |
                    +------+------+ 
                           |
                           v
                  +------------------+
                  | Apache Iceberg   |
                  |  (REST Catalog)  |
                  +--------+---------+
                           |
                           v
                       +--------+
                       | MinIO  |
                       +--------+

        +-----------------------------------+
        | PySpark / Polars / Jupyter        |
        | (IngestÃ£o, TransformaÃ§Ã£o, EDA)    |
        +-----------------------------------+
```

---

## ğŸ¯ Objetivo do Projeto

Construir um **Data Lakehouse moderno** para:

* Centralizar dados financeiros pessoais (extratos bancÃ¡rios)
* Consolidar dados operacionais de uma empresa de venda de carros
* Testar arquitetura analÃ­tica baseada em Iceberg
* Explorar versionamento de dados, schema evolution e time travel
* Criar dashboards financeiros e operacionais

---

## ğŸ“‚ DomÃ­nios de Dados

### 1ï¸âƒ£ Financeiro Pessoal

* Extratos bancÃ¡rios (CSV)
* TransaÃ§Ãµes categorizadas
* Receitas e despesas
* ConsolidaÃ§Ã£o mensal
* AnÃ¡lises:

  * Fluxo de caixa
  * Despesas por categoria
  * EvoluÃ§Ã£o patrimonial

---

### 2ï¸âƒ£ Empresa de Venda de Carros

* Estoque de veÃ­culos
* HistÃ³rico de vendas
* Margem por veÃ­culo
* Ticket mÃ©dio
* Tempo mÃ©dio em estoque
* Receita mensal

---

## ğŸ§Š Por que Apache Iceberg?

Uso do Iceberg permite:

* ACID sobre object storage
* Versionamento de dados
* Time travel queries
* Schema evolution
* Particionamento oculto
* Merge / Upsert eficientes

Exemplo de consulta time travel no Trino:

```sql
SELECT *
FROM vendas FOR VERSION AS OF 123456789;
```

---

## ğŸ”„ Fluxo de IngestÃ£o

### ğŸŸ¢ Camada Bronze

* Dados brutos ingeridos no MinIO
* Formato original preservado

### ğŸŸ¡ Camada Silver

* Limpeza
* NormalizaÃ§Ã£o
* ConversÃ£o de tipos
* PadronizaÃ§Ã£o de datas

### ğŸ”µ Camada Gold

* Tabelas analÃ­ticas
* AgregaÃ§Ãµes
* MÃ©tricas de negÃ³cio

Processamento realizado com:

* PySpark (grandes volumes)
* Polars (processamento rÃ¡pido local)
* Jupyter para EDA

---

## ğŸ“Š Consultas via Trino

O Trino Ã© utilizado para:

* Consultas SQL federadas
* IntegraÃ§Ã£o com Metabase
* AnÃ¡lises ad hoc
* ValidaÃ§Ã£o de dados transformados

Exemplo:

```sql
SELECT 
    date_trunc('month', data_venda) AS mes,
    SUM(valor_venda) AS receita_total
FROM gold.vendas
GROUP BY 1
ORDER BY 1;
```

---

## ğŸ“ˆ Dashboards (Metabase)

PainÃ©is criados:

### Financeiro Pessoal

* Despesas por categoria
* Receita vs Despesa
* EvoluÃ§Ã£o mensal

### Empresa

* Receita mensal
* Margem por veÃ­culo
* Giro de estoque
* Ticket mÃ©dio

---

## âš™ï¸ Estrutura do Projeto

```text
datalake/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ minio/
â”‚   â”œâ”€â”€ trino/
â”‚   â”œâ”€â”€ iceberg-rest/
â”‚   â””â”€â”€ metabase/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ financeiro/
â”‚   â””â”€â”€ empresa/
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â””â”€â”€ sql/
    â”œâ”€â”€ ddl/
    â””â”€â”€ analytics/
```

---

## ğŸ§ª Aprendizados TÃ©cnicos

* ImplementaÃ§Ã£o prÃ¡tica de arquitetura Lakehouse
* Uso de Iceberg com REST catalog
* IntegraÃ§Ã£o Trino + Iceberg + MinIO
* EstratÃ©gias de particionamento
* EvoluÃ§Ã£o de schema
* OtimizaÃ§Ã£o de consultas
* EstratÃ©gias de modelagem analÃ­tica

---

## ğŸš€ Como Executar

### 1ï¸âƒ£ Subir infraestrutura

```bash
docker-compose up -d
```

ServiÃ§os disponÃ­veis:

* MinIO â†’ [http://localhost:9000](http://localhost:9000)
* Trino â†’ [http://localhost:8080](http://localhost:8080)
* Metabase â†’ [http://localhost:3000](http://localhost:3000)

---

### 2ï¸âƒ£ Criar tabelas Iceberg

Executar scripts em `sql/ddl/` via Trino.

---

### 3ï¸âƒ£ Rodar ingestÃµes

Executar notebooks ou jobs PySpark:

```bash
python jobs/silver/transform_financeiro.py
```

---

## ğŸ“Œ PrÃ³ximos Passos

* [ ] Implementar CDC
* [ ] AutomaÃ§Ã£o com Airflow
* [ ] Camada de qualidade de dados
* [ ] Testes de performance
* [ ] MÃ©tricas de observabilidade
* [ ] Deploy em cloud

---

## ğŸ§  MotivaÃ§Ã£o

Projeto criado como laboratÃ³rio prÃ¡tico para:

* Dominar arquitetura Lakehouse
* Consolidar conhecimentos em engenharia de dados
* Aplicar conceitos modernos em um cenÃ¡rio real
* Criar um ambiente analÃ­tico prÃ³prio e controlado

---

## ğŸ“œ LicenÃ§a

Uso pessoal / educacional.

## Baseado em: 
- [Engenharia de Dados na PrÃ¡tica: Criando um Data Lake em casa!](https://www.youtube.com/watch?v=ntp-OfixCm4)

Artigos utilizados para realizar o projeto: 

- Streamlining Big Data with Spark: Writing and Reading Delta Lake Format on MinIO-S3 Storage - medium 
- Setting Up Trino with Hive to Query Delta Lake Data on MinIO: A Scalable Big Data Solution
- https://www.datalib.com.br/post/como-instalar-um-cluster-do-apache-spark-no-docker-desktop-utilizando-compose
- https://blog.min.io/a-developers-introduction-to-apache-iceberg-using-minio/


Apache spark + minio
- trino
- Hive
- postgres
- superset

Todos esses serviÃ§os rodam em uma docker

conceito S3 no minio

CriaÃ§Ã£o de um datalake

Vou tentar colocar tudo em container docker

# O catÃ¡logo 'iceberg' jÃ¡ estÃ¡ configurado no spark-defaults.conf
spark = SparkSession.builder.getOrCreate()

# Criar namespace (database)
spark.sql("CREATE NAMESPACE IF NOT EXISTS iceberg.db")

# Criar tabela
spark.sql("""
CREATE TABLE IF NOT EXISTS iceberg.db.teste (
  id bigint,
  nome string
) USING iceberg
""")

# Inserir dados de teste
spark.sql("""
INSERT INTO iceberg.db.teste VALUES 
  (1, 'Alice'),
  (2, 'Bob'),
  (3, 'Carlos')
""")

# Consultar
spark.sql("SELECT * FROM iceberg.db.teste").show()
